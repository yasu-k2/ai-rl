{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AudioVisualMaze.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRfejBRdbBCEXPILa6avZ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasu-k2/multimodal-active-inference/blob/main/AudioVisualMaze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwJgFYGhbgpZ"
      },
      "outputs": [],
      "source": [
        "!pip install pygame\n",
        "# !pip install inferactively-pymdp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "jdaOeAp4k3a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/infer-actively/pymdp.git"
      ],
      "metadata": {
        "id": "pkymecKtklTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pymdp\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e ./\n",
        "%cd .."
      ],
      "metadata": {
        "id": "0Tb5qPfrk6qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i -e 's/actions\\[factor\\]\\]/int(actions\\[factor\\])]/g' pymdp/pymdp/learning.py"
      ],
      "metadata": {
        "id": "5yypbq5ZlYnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yasu-k2/multimodal-active-inference.git"
      ],
      "metadata": {
        "id": "DuNshCcCbotO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd multimodal-active-inference"
      ],
      "metadata": {
        "id": "Vu5p8xWDbzfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !git pull origin main"
      ],
      "metadata": {
        "id": "76-kLuhud1md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "3LLRcX98sLSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pymdp\n",
        "from pymdp import maths\n",
        "from pymdp import utils\n",
        "from pymdp.agent import Agent\n",
        "\n",
        "from AudioVisualMazeEnv import print_maze, fn_create_maze, create_maze, AudioVisualMazeEnv"
      ],
      "metadata": {
        "id": "arGntxrNsOm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "eZFCq_WbsG6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Visual Maze Setup"
      ],
      "metadata": {
        "id": "J4maZVyLR0v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maze_dir = './'\n",
        "n_div_sound = 7\n",
        "maze_height = 7\n",
        "maze_width = 7\n",
        "create = True\n",
        "seed = 4\n",
        "\n",
        "# assert (maze_width < 32), ValueError('`maze_width` must be smaller than 32')\n",
        "env = AudioVisualMazeEnv(n_div_sound=n_div_sound, maze_height=maze_height, maze_width=maze_width,\n",
        "                         create=create, seed=seed, start_pos=(1,1), end_pos='LowerRight', DIR=maze_dir)\n",
        "maze_array = copy.deepcopy(env.maze_array)"
      ],
      "metadata": {
        "id": "_Dop_PjCb3tU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"STAY\"]\n",
        "n_actions = len(actions)"
      ],
      "metadata": {
        "id": "x56UhT_Od6YD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 3\n",
        "obs = env.reset()\n",
        "print('obs :', obs[1])\n",
        "print_maze(obs[0])\n",
        "for t in range(T):\n",
        "  action_index = np.random.choice(len(actions))\n",
        "  # action_index = 1\n",
        "  action_label = actions[action_index]\n",
        "  obs, rewards, done, info = env.step(action_label)\n",
        "  print('=====Time {}====='.format(t+1))\n",
        "  env.render()\n",
        "  print(env.current_state)  # [Y, X, audio]\n",
        "  print('obs :', obs[1])\n",
        "  print_maze(obs[0])"
      ],
      "metadata": {
        "id": "sVHzoi864wdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Active Inference Agent"
      ],
      "metadata": {
        "id": "q3oZulrkRxDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grid(grid_locations, num_y=21, num_x=21):\n",
        "  grid_heatmap = np.zeros((num_y, num_x))\n",
        "  for linear_idx, location in enumerate(grid_locations):\n",
        "    y, x = location\n",
        "    grid_heatmap[y, x] = linear_idx\n",
        "  sns.heatmap(grid_heatmap, annot=True, cbar=False, fmt='.0f', cmap='crest')\n",
        "\n",
        "def plot_point_on_grid(state_vector, grid_locations, num_y=21, num_x=21):\n",
        "  state_index = np.where(state_vector)[0][0]\n",
        "  print(np.where(state_vector))\n",
        "  y, x = grid_locations[state_index]\n",
        "  grid_heatmap = np.zeros((num_y, num_x))\n",
        "  grid_heatmap[y,x] = 1.0\n",
        "  sns.heatmap(grid_heatmap, cbar=False, fmt='.0f')\n",
        "\n",
        "def plot_likelihood(matrix, title_str=\"\"):\n",
        "  if not np.isclose(matrix.sum(axis=0), 1.0).all():\n",
        "    raise ValueError(\"Distrbution not column-normalized.\")\n",
        "  sns.heatmap(matrix, cmap='OrRd', vmin=0.0, vmax=1.0)\n",
        "  plt.xticks(range(A.shape[1]))\n",
        "  plt.yticks(range(A.shape[0]))\n",
        "  plt.title(title_str)\n",
        "\n",
        "def plot_beliefs(belief_dist, title_str=\"\"):\n",
        "  if not np.isclose(belief_dist.sum(), 1.0):\n",
        "    raise ValueError(\"Distribution not normalized.\")\n",
        "  plt.grid(zorder=0)\n",
        "  plt.bar(range(belief_dist.shape[0], belief_dist, color='r', zorder=3))\n",
        "  plt.xticks(range(belief_dist.shape[0]))\n",
        "  plt.title(title_str)"
      ],
      "metadata": {
        "id": "lGpTjjhg7gt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_oracle_B_matrix(maze_array, grid_locations, actions):\n",
        "  maze_height, maze_width = maze_array.shape\n",
        "  n_states = len(grid_locations)\n",
        "  n_action = len(actions)\n",
        "  B = np.zeros((n_states, n_states, n_actions))\n",
        "  for action_id, action_label in enumerate(actions):\n",
        "    for curr_state, grid_location in enumerate(grid_locations):\n",
        "      y, x = grid_location\n",
        "      if action_label == \"UP\":\n",
        "        next_y = y - 1 if y > 0 else y\n",
        "        next_x = x\n",
        "      elif action_label == \"DOWN\":\n",
        "        next_y = y + 1 if y < (maze_height-1) else y\n",
        "        next_x = x\n",
        "      elif action_label == \"LEFT\":\n",
        "        next_x = x - 1 if x > 0 else x\n",
        "        next_y = y\n",
        "      elif action_label == \"RIGHT\":\n",
        "        next_x = x + 1 if x < (maze_width-1) else x\n",
        "        next_y = y\n",
        "      elif action_label == \"STAY\":\n",
        "        next_x = x\n",
        "        next_y = y\n",
        "\n",
        "      # Blocked by wall\n",
        "      if maze_array[next_y, next_x] == 1:\n",
        "        next_x = x\n",
        "        next_y = y\n",
        "\n",
        "      next_location = (next_y, next_x)\n",
        "      next_state = grid_locations.index(next_location)\n",
        "      B[next_state, curr_state, action_id] = 1.0\n",
        "  return B"
      ],
      "metadata": {
        "id": "yzO5dYPfMZPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0(path), 1(wall), 2(start), 3(goal)\n",
        "def categorize_maze_obs(maze_array, gy, gx):\n",
        "  maze_height, maze_width = maze_array.shape\n",
        "  if ((0 < gy < (maze_height - 1)) and (0 < gx < (maze_width - 1))):\n",
        "    maze_obs = maze_array[(gy-1):(gy+2), (gx-1):(gx+2)]\n",
        "  else:\n",
        "    if gy == 0:\n",
        "      maze_obs = maze_array[gy:(gy+2)]\n",
        "      maze_obs = np.vstack([np.ones((1, maze_width)), maze_obs])\n",
        "    elif gy == (maze_height - 1):\n",
        "      maze_obs = maze_array[(gy-1):(gy+1)]\n",
        "      maze_obs = np.vstack([maze_obs, np.ones((1, maze_width))])\n",
        "    else:\n",
        "      maze_obs = maze_array[(gy-1):(gy+2)]\n",
        "    if gx == 0:\n",
        "      maze_obs = maze_obs[:, gx:(gx+2)]\n",
        "      maze_obs = np.hstack([np.ones((3, 1)), maze_obs])\n",
        "    elif gx == (maze_width - 1):\n",
        "      maze_obs = maze_obs[:, (gx-1):(gx+1)]\n",
        "      maze_obs = np.hstack([maze_obs, np.ones((3, 1))])\n",
        "    else:\n",
        "      maze_obs = maze_obs[:, (gx-1):(gx+2)]\n",
        "  # print(maze_obs.shape)\n",
        "  # maze_obs = maze_obs.reshape(-1)\n",
        "  maze_obs = np.array([maze_obs[0,1], maze_obs[1,0], maze_obs[1,2], maze_obs[2,1]])\n",
        "  maze_obs_index = sum([int(mo==1.) * (2**moi) for moi, mo in enumerate(maze_obs)])\n",
        "  # print(maze_obs_index)\n",
        "  return maze_obs_index"
      ],
      "metadata": {
        "id": "aO_oFB-OdN5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_locations = list(itertools.product(range(maze_height), range(maze_width)))\n",
        "# print(len(grid_locations), grid_locations)\n",
        "# plot_grid(grid_locations)\n",
        "starting_location = (1, 1)  # upper left\n",
        "starting_state_index = grid_locations.index(starting_location)\n",
        "desired_location = (maze_height-2, maze_width-2)  # lower right\n",
        "desired_location_index = grid_locations.index(desired_location)"
      ],
      "metadata": {
        "id": "pKvhzxuA8C4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define state space\n",
        "n_location_states = len(grid_locations)  # current position\n",
        "# no `maze_state` for blocking(0) or non-blocking(1)\n",
        "# first consider a known maze state. encoded in B matrix.\n",
        "n_states = [n_location_states]\n",
        "n_factors = len(n_states)\n",
        "n_controls = [n_actions]\n",
        "print(n_states, n_controls)\n",
        "\n",
        "# Define observation space\n",
        "n_location_observations = len(grid_locations)  # current position\n",
        "# n_maze_observations = 2 ** (3 * 3)  # surrounding tiles\n",
        "n_maze_observations = 2 ** 4  # surrounding tiles (4 ways)\n",
        "n_sound_observations = n_div_sound  # sound cue intensity\n",
        "n_reward_observations = 2  # reward or no reward\n",
        "n_obs = [n_location_observations, n_maze_observations, n_sound_observations, n_reward_observations]\n",
        "n_modalities = len(n_obs)\n",
        "print(n_obs)"
      ],
      "metadata": {
        "id": "RQMIltOCSZJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the A array\n",
        "A = utils.obj_array(n_modalities)\n",
        "\n",
        "A_location = np.zeros((n_location_observations, *n_states))\n",
        "# print(A_location.shape)\n",
        "A_location[:, :] = np.eye(n_location_states)\n",
        "print(utils.is_normalized(A_location))\n",
        "print(A_location)\n",
        "A[0] = A_location\n",
        "\n",
        "A_maze = np.zeros((n_maze_observations, *n_states))\n",
        "for gli, gl in enumerate(grid_locations):\n",
        "  gy, gx = gl\n",
        "  maze_obs_index = categorize_maze_obs(maze_array, gy, gx)\n",
        "  A_maze[maze_obs_index, gli] = 1.0\n",
        "print(utils.is_normalized(A_maze))\n",
        "print(A_maze)\n",
        "A[1] = A_maze\n",
        "\n",
        "A_sound = np.zeros((n_sound_observations, *n_states))\n",
        "# print(env.bins)\n",
        "for gli, gl in enumerate(grid_locations):\n",
        "  sli = env.compute_sound(gl[0], gl[1])\n",
        "  A_sound[sli, gli] = 1.0  # reliable\n",
        "  # A_sound[:, gli] = maths.softmax(2. * utils.onehot(sli, n_div_sound))  # ambiguous\n",
        "print(utils.is_normalized(A_sound))\n",
        "print(A_sound)\n",
        "A[2] = A_sound\n",
        "\n",
        "A_reward = np.zeros((n_reward_observations, *n_states))\n",
        "# The agent knows the rewarding location\n",
        "A_reward[0, :] = 1.0\n",
        "A_reward[0, desired_location_index] = 0.0\n",
        "A_reward[1, desired_location_index] = 1.0\n",
        "print(utils.is_normalized(A_reward))\n",
        "print(A_reward)\n",
        "A[3] = A_reward"
      ],
      "metadata": {
        "id": "0IhupMId8U7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the B array\n",
        "B = utils.obj_array(n_factors)\n",
        "B_location = create_oracle_B_matrix(maze_array, grid_locations, actions)  # perfect\n",
        "# B_location = utils.random_B_matrix(n_states, n_controls)  # random\n",
        "print(utils.is_normalized(B_location))\n",
        "print(B_location)\n",
        "B[0] = B_location"
      ],
      "metadata": {
        "id": "0aiOGCEzSdMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the C array\n",
        "C = utils.obj_array_zeros(n_obs)\n",
        "\n",
        "C_location = np.zeros(n_location_observations)\n",
        "C_location[desired_location_index] = 1.0\n",
        "# print(C_location)\n",
        "# print(utils.is_normalized(C_location))\n",
        "\n",
        "# C_maze = np.zeros(n_maze_observations)\n",
        "\n",
        "C_sound = np.zeros(n_sound_observations)\n",
        "# desired_sound_index = 0  # smallest sound\n",
        "# C_sound[desired_sound_index] = 1.0\n",
        "sound_pref = 1. - np.linspace(0., 1., n_div_sound)  # smaller the better\n",
        "C_sound[:] = sound_pref\n",
        "print(C_sound)\n",
        "# print(utils.is_normalized(C_sound))\n",
        "\n",
        "C_reward = np.zeros(n_reward_observations)\n",
        "C_reward[1] = 5.0\n",
        "print(C_reward)\n",
        "# print(utils.is_normalized(C_reward))\n",
        "\n",
        "# Choose your preference\n",
        "# C[0] = C_location\n",
        "# C[1] = C_maze\n",
        "C[2] = C_sound\n",
        "C[3] = C_reward"
      ],
      "metadata": {
        "id": "L57jsxB6SfWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the D array\n",
        "D = utils.obj_array(n_factors)\n",
        "\n",
        "D_location = utils.onehot(starting_state_index, n_location_states)\n",
        "# plot_point_on_grid(D_location, grid_locations)\n",
        "print(utils.is_normalized(D_location))\n",
        "print(D_location)\n",
        "D[0] = D_location"
      ],
      "metadata": {
        "id": "Kz36zl0zShEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_active_inference_loop(my_agent, my_env, T=3):\n",
        "  obs = my_env.reset()\n",
        "  reward = 0\n",
        "  for t in range(T):\n",
        "    my_env.render()\n",
        "    print('obs :', obs[1])\n",
        "    print_maze(obs[0])\n",
        "  \n",
        "    location = (my_env.current_state[0], my_env.current_state[1])\n",
        "    maze_obs_index = categorize_maze_obs(maze_array, *location)\n",
        "    sound = obs[1]\n",
        "    obs_agent = [grid_locations.index(location), maze_obs_index, sound, reward]\n",
        "    qs = my_agent.infer_states(obs_agent)\n",
        "    q_pi, efe = my_agent.infer_policies()\n",
        "    action_id = my_agent.sample_action()\n",
        "    action_index = int(action_id[0])\n",
        "\n",
        "    action_label = actions[action_index]\n",
        "    obs, reward, done, info = my_env.step(action_label)\n",
        "    print('=====Time {}====='.format(t+1))\n",
        "\n",
        "    print(f'Action at time {t+1}: {action_label}')\n",
        "    print(f'Reward at time {t+1}: {reward}')"
      ],
      "metadata": {
        "id": "xWIFal91PbXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_len = 4\n",
        "controllable_indices = [0]\n",
        "action_selection = 'stochastic'  # 'deterministic', 'stochastic'"
      ],
      "metadata": {
        "id": "b_VmUCY7ECKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_agent = Agent(A=A, B=B, C=C, D=D,\n",
        "                 policy_len=policy_len,\n",
        "                 control_fac_idx=controllable_indices,\n",
        "                 action_selection=action_selection)"
      ],
      "metadata": {
        "id": "YvBA04_xhPH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 1\n",
        "run_active_inference_loop(my_agent, env, T=T)"
      ],
      "metadata": {
        "id": "JO_aYb02Sb9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ActInf with learning\n",
        "\n",
        "Choose the matrix you want to learn. Learning A in the example below."
      ],
      "metadata": {
        "id": "GbBHyfagdlp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agent(A=A, pA=pA, modalities_to_learn=learnable_modalities, lr_pA=1.0)\n",
        "#  agent.update_A(obs): update_obs_likelihood_dirichlet(pA, A, obs, qs)\n",
        "pA = utils.dirichlet_like(A, scale=1.0)\n",
        "# TODO: do something with pA\n",
        "A_gm = utils.norm_dist_obj_arr(pA)\n",
        "\n",
        "# Agent(B=B, pB=pB, factors_to_learn=learnable_factors, lr_pB=1.0)\n",
        "#   agent.update_B(qs_prev): update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev)\n",
        "pB = utils.dirichlet_like(B, scale=1.0)\n",
        "# TODO: do something with pB\n",
        "B_gm = utils.norm_dist_obj_arr(pB)\n",
        "\n",
        "# Agent(D=D, pD=pD, factors_to_learn=learnable_factors, lr_pD=1.0)\n",
        "#   agent.update_D(qs_t0=None): update_state_prior_dirichlet(pD, qs)\n",
        "pD = utils.dirichlet_like(D, scale=1.0)\n",
        "# TODO: do something with pD\n",
        "D_gm = utils.norm_dist_obj_arr(pD)\n",
        "\n",
        "# TODO: specify indices\n",
        "learnable_modalities = [1]\n",
        "A_gm_l = copy.deepcopy(A)\n",
        "for lm in learnable_modalities:\n",
        "  # print([n_obs[lm]] + n_states)\n",
        "  A_gm_l[lm] = utils.norm_dist(np.random.rand(*[n_obs[lm]] + n_states))  # random\n",
        "  # A_gm_l[lm] = A_gm[lm].copy()  # default\n",
        "\n",
        "# TODO: specify indices\n",
        "learnable_factors = [0]\n",
        "B_gm_l = copy.deepcopy(B)\n",
        "for lf in learnable_factors:\n",
        "  # print(n_states[lf], n_states[lf], n_controls[lf])\n",
        "  B_gm_l[lf] = utils.norm_dist(np.random.rand(n_states[lf], n_states[lf], n_controls[lf]))  # random\n",
        "  # B_gm_l[lf] = B_gm[lf].copy()  # default"
      ],
      "metadata": {
        "id": "v3TBkIZ5T4zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn only A\n",
        "my_agent_a = Agent(A=A_gm_l, B=B, C=C, D=D,\n",
        "                   pA=pA, lr_pA=1.0,\n",
        "                   policy_len=policy_len,\n",
        "                   control_fac_idx=controllable_indices,\n",
        "                   action_selection=action_selection,\n",
        "                   modalities_to_learn=learnable_modalities,\n",
        "                   use_param_info_gain=True)"
      ],
      "metadata": {
        "id": "tsYibG9oYVMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn only B\n",
        "my_agent_b = Agent(A=A, B=B_gm_l, C=C, D=D,\n",
        "                   pB=pB, lr_pB=1.0,\n",
        "                   policy_len=policy_len,\n",
        "                   control_fac_idx=controllable_indices,\n",
        "                   action_selection=action_selection,\n",
        "                   factors_to_learn=learnable_factors,\n",
        "                   use_param_info_gain=True)"
      ],
      "metadata": {
        "id": "VWxpdp2AYYxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn A and B\n",
        "my_agent_ab = Agent(A=A_gm_l, B=B_gm_l, C=C, D=D,\n",
        "                    pA=pA, lr_pA=1.0, pB=pB, lr_pB=1.0,\n",
        "                    policy_len=policy_len,\n",
        "                    control_fac_idx=controllable_indices,\n",
        "                    action_selection=action_selection,\n",
        "                    modalities_to_learn=learnable_modalities,\n",
        "                    factors_to_learn=learnable_factors,\n",
        "                    use_param_info_gain=True)"
      ],
      "metadata": {
        "id": "QgjPk8DXYfM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_active_inference_loop_with_learning(my_agent, my_env, T=3, learn_A=False, learn_B=False):\n",
        "  obs = my_env.reset()\n",
        "  reward = 0\n",
        "  # qs = [utils.norm_dist(np.ones(n_location_states))]  # tmp\n",
        "  qs = [D_location]  # tmp\n",
        "  for t in range(T):\n",
        "    # env.render()\n",
        "    location = (my_env.current_state[0], my_env.current_state[1])\n",
        "    maze_obs_index = categorize_maze_obs(maze_array, *location)\n",
        "    sound = obs[1]\n",
        "    obs_agent = [grid_locations.index(location), maze_obs_index, sound, reward]\n",
        "    qs_prev = qs.copy()\n",
        "    qs = my_agent.infer_states(obs_agent)\n",
        "    if learn_A:\n",
        "      pA_t = my_agent.update_A(obs_agent)  # update A via pA\n",
        "    print(qs_prev, qs)\n",
        "    if learn_B and (t > 0):\n",
        "      pB_t = my_agent.update_B(qs_prev)  # update B via pB\n",
        "    q_pi, efe = my_agent.infer_policies()\n",
        "    action_id = my_agent.sample_action()\n",
        "\n",
        "    action_index = int(action_id[0])\n",
        "    action_label = actions[action_index]\n",
        "    obs, reward, done, info = my_env.step(action_label)    "
      ],
      "metadata": {
        "id": "FifO7k5idCta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 1\n",
        "run_active_inference_loop_with_learning(my_agent_a, env, T=T, learn_A=True)"
      ],
      "metadata": {
        "id": "uXPnUiowd0lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 1\n",
        "run_active_inference_loop_with_learning(my_agent_b, env, T=T, learn_B=True)"
      ],
      "metadata": {
        "id": "oend7uNMbjEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T = 1\n",
        "run_active_inference_loop_with_learning(my_agent_ab, env, T=T, learn_A=True, learn_B=True)"
      ],
      "metadata": {
        "id": "TkGbtL-xbk_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4NOmufeLtfvg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}